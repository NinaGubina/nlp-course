{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **T1.** Pick an article from wiki (using requests package or any other) in XML or HTML format and write a simple parser using regular expressions that does following:\n",
    "1. Replace all boldface and italic tags <b></b> and  <i></i> with the content between them. E.g. \"<b>machine learning</b>\" should become \"machine learning\".\n",
    "2. Replace all hyperref tags with the content inside: e.g. \"<a href=...>content</a>\" should become \"content\".\n",
    "3. Do same as in 2 for <span ...><\\span> and <sup></sup> tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(paragraph, n):\n",
    "    p = paragraph[n]\n",
    "\n",
    "    href_pattern = r\"<a href=[^>]+>([^<]+)</a>\"\n",
    "    p = re.sub(href_pattern, r\"\\1\", p)\n",
    "\n",
    "    boldface_pattern = r\"<b>([^<]+)</b>\"\n",
    "    p = re.sub(boldface_pattern, r\"\\1\", p)\n",
    "\n",
    "    italic_pattern = r\"<i>([^<]+)</i>\"\n",
    "    p = re.sub(italic_pattern, r\"\\1\", p)\n",
    "\n",
    "    span_pattern = r\"<span\\s[^<]+</span>\"\n",
    "    p = re.sub(span_pattern, \"\", p)\n",
    "\n",
    "    sup_pattern = r\"<sup\\s[^<]+</sup>\"\n",
    "    p = re.sub(sup_pattern, \"\", p)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url=\"https://en.wikipedia.org/wiki/Machine_learning\")\n",
    "wiki_text = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = re.findall(r\"<p>(.*)\\n\", wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_num = range(50)\n",
    "\n",
    "with open('text.txt', 'w') as f:\n",
    "    for n in paragraphs_num:\n",
    "        p = clean_text(paragraph, n)\n",
    "\n",
    "        #print('BEFORE: ', paragraph[n])\n",
    "        #print('AFTER: ', p)\n",
    "\n",
    "        f.write(p)\n",
    "        #f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **T2.** Use regular expressions to split text into sentences and sentences into tokens. Compare your results with the results of tokenizer from NLTK or SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = clean_text(paragraph, 0)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'don', 't', 'know', 'about', 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "sent_my = \"I don't know about New York\"\n",
    "p_word_regex = re.findall(r\"([\\w]+|\\(|\\)|\\.|,)\", sent_my)\n",
    "print(p_word_regex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', \"n't\", 'know', 'about', 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "p_word_nltk = word_tokenize(sent_my)\n",
    "print(p_word_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sent_regex = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sent_my)\n",
    "print(p_sent_regex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.', 'Recently, artificial neural networks have been able to surpass many previous approaches in performance.']\n",
      "['Machine', 'learning', '(', 'ML', ')', 'is', 'a', 'field', 'of', 'study', 'in', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'development', 'and', 'study', 'of', 'statistical', 'algorithms', 'that', 'can', 'learn', 'from', 'data', 'and', 'generalize', 'to', 'unseen', 'data', ',', 'and', 'thus', 'perform', 'tasks', 'without', 'explicit', 'instructions', '.']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# regex\n",
    "# sentences\n",
    "p_sent_regex = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", p)\n",
    "print(p_sent_regex) \n",
    "\n",
    "# words\n",
    "p_word_regex = re.findall(r\"([\\w]+|\\(|\\)|\\.|,)\", p_sent_regex[0])\n",
    "print(p_word_regex) \n",
    "print(len(p_word_regex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.', 'Recently, artificial neural networks have been able to surpass many previous approaches in performance.']\n",
      "['Machine', 'learning', '(', 'ML', ')', 'is', 'a', 'field', 'of', 'study', 'in', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'development', 'and', 'study', 'of', 'statistical', 'algorithms', 'that', 'can', 'learn', 'from', 'data', 'and', 'generalize', 'to', 'unseen', 'data', ',', 'and', 'thus', 'perform', 'tasks', 'without', 'explicit', 'instructions', '.']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "# sentences\n",
    "p_sent_nltk = sent_tokenize(p)\n",
    "print(p_sent_nltk) \n",
    "\n",
    "# words\n",
    "p_word_nltk = word_tokenize(p_sent_nltk[0])\n",
    "print(p_word_nltk) \n",
    "print(len(p_word_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions., Recently, artificial neural networks have been able to surpass many previous approaches in performance.]\n",
      "['Machine', 'learning', '(', 'ML', ')', 'is', 'a', 'field', 'of', 'study', 'in', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'development', 'and', 'study', 'of', 'statistical', 'algorithms', 'that', 'can', 'learn', 'from', 'data', 'and', 'generalize', 'to', 'unseen', 'data', ',', 'and', 'thus', 'perform', 'tasks', 'without', 'explicit', 'instructions', '.']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# SpaCy\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "# sentences\n",
    "p_spicy = nlp_spacy(p)\n",
    "p_sent_spicy = [sent for sent in p_spicy.sents]\n",
    "print(p_sent_spicy) \n",
    "\n",
    "# words\n",
    "p_word_spicy = [token.text for token in p_sent_spicy[0]]\n",
    "print(p_word_spicy) \n",
    "print(len(p_word_spicy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **T3.** Normalize your text using NLTK or SpaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = clean_text(paragraph, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.', 'Recently, artificial neural networks have been able to surpass many previous approaches in performance.']\n",
      "['Machine', 'learning', '(', 'ML', ')', 'is', 'a', 'field', 'of', 'study', 'in', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'development', 'and', 'study', 'of', 'statistical', 'algorithms', 'that', 'can', 'learn', 'from', 'data', 'and', 'generalize', 'to', 'unseen', 'data', ',', 'and', 'thus', 'perform', 'tasks', 'without', 'explicit', 'instructions', '.', 'Recently', ',', 'artificial', 'neural', 'networks', 'have', 'been', 'able', 'to', 'surpass', 'many', 'previous', 'approaches', 'in', 'performance', '.']\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "# sentences\n",
    "p_sent_nltk = sent_tokenize(p)\n",
    "print(p_sent_nltk) \n",
    "\n",
    "# words\n",
    "p_word_nltk = []\n",
    "for sentence in p_sent_nltk:\n",
    "    p_word_nltk += [word for word in word_tokenize(sentence)]\n",
    "\n",
    "print(p_word_nltk) \n",
    "print(len(p_word_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machin', 'learn', '(', 'ml', ')', 'is', 'a', 'field', 'of', 'studi', 'in', 'artifici', 'intellig', 'concern', 'with', 'the', 'develop', 'and', 'studi', 'of', 'statist', 'algorithm', 'that', 'can', 'learn', 'from', 'data', 'and', 'gener', 'to', 'unseen', 'data', ',', 'and', 'thu', 'perform', 'task', 'without', 'explicit', 'instruct', '.', 'recent', ',', 'artifici', 'neural', 'network', 'have', 'been', 'abl', 'to', 'surpass', 'mani', 'previou', 'approach', 'in', 'perform', '.']\n",
      "57\n",
      "['machin', 'learn', '(', 'ml', ')', 'field', 'studi', 'artifici', 'intellig', 'concern', 'develop', 'studi', 'statist', 'algorithm', 'learn', 'data', 'gener', 'unseen', 'data', ',', 'thu', 'perform', 'task', 'without', 'explicit', 'instruct', '.', 'recent', ',', 'artifici', 'neural', 'network', 'abl', 'surpass', 'mani', 'previou', 'approach', 'perform', '.']\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "# stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in p_word_nltk]\n",
    "print(stemmed_words)\n",
    "print(len(stemmed_words))\n",
    "#stemmed_paragraphs = [' '.join(stemmed_words[i:i+10])for i in range(0, len(stemmed_words), 10)]\n",
    "#print(stemmed_paragraphs)\n",
    "\n",
    "# lemmatization \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words if word not in set (stopwords.words('english'))]\n",
    "print(lemmatized_words)\n",
    "print(len(lemmatized_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machin learn ( ml ) field studi artifici intellig concern develop studi statist algorithm learn data gener unseen data , thu perform task without explicit instruct .', 'recent , artifici neural network abl surpass mani previou approach perform .']\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "p_sent_nltk_normalized = []\n",
    "\n",
    "for i in range(len(p_sent_nltk)):\n",
    "    p_word_nltk = [word for word in word_tokenize(p_sent_nltk[i])]\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in p_word_nltk]\n",
    "\n",
    "    # lemmatization \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words if word not in set (stopwords.words('english'))]\n",
    "\n",
    "    p_sent_nltk_normalized.append(' '.join(lemmatized_words))\n",
    "\n",
    "print(p_sent_nltk_normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
